{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae5dfd9",
   "metadata": {},
   "source": [
    "- GSE251676 dataset as an example \n",
    "- Dataset can be downloaded here: https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE251676&format=file\n",
    "- The reference genome is: GCF_000011065.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7333fe3",
   "metadata": {},
   "source": [
    "### calculate the normalized gene expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cac1d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import esm\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "from Bio.SeqRecord import SeqRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df078ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing your gzipped files\n",
    "data_folder = '../example_data/GSE251676/' \n",
    "\n",
    "# Function to calculate TPM using gene lengths\n",
    "def calculate_tpm(counts, lengths):\n",
    "    rpk = counts / (lengths / 1e3)  # Reads per kilobase\n",
    "    tpm = (rpk / np.sum(rpk)) * 1e6  # Normalize to per million\n",
    "    return tpm\n",
    "\n",
    "# Initialize an empty dictionary to store data from each file\n",
    "data_dict = {}\n",
    "\n",
    "# Process each gzipped TSV file in the folder\n",
    "for file_name in os.listdir(data_folder):\n",
    "    if file_name.endswith('.gz'):\n",
    "        file_path = os.path.join(data_folder, file_name)\n",
    "        \n",
    "        # Extract file name without extension to use as column name\n",
    "        column_name = file_name.split('.')[0]\n",
    "        \n",
    "        # Open and read the gzipped file\n",
    "        with gzip.open(file_path, 'rt') as f:\n",
    "            # Read into a DataFrame\n",
    "            df = pd.read_csv(f, sep='\\t', comment='#')\n",
    "            \n",
    "            # Identify the column containing counts dynamically\n",
    "            count_column = [col for col in df.columns if col.endswith('_sorted.sam')][0]\n",
    "            df = df[['Geneid', 'Length', count_column]]  # Select relevant columns\n",
    "            df.columns = ['gene_id', 'length', 'count']  # Standardize column names\n",
    "            \n",
    "        # Calculate TPM values for the current file\n",
    "        df['tpm'] = calculate_tpm(df['count'].values, df['length'].values)\n",
    "        \n",
    "        # Log-transform the TPM values\n",
    "        df['log_tpm'] = np.log1p(df['tpm'])  # log1p handles log(0) by log(1 + x)\n",
    "        \n",
    "        # Store the log-transformed values in the dictionary\n",
    "        data_dict[column_name] = df.set_index('gene_id')['log_tpm']\n",
    "\n",
    "# Combine all columns into a single DataFrame based on gene_id\n",
    "combined_df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Z-score normalization for each row (gene)\n",
    "combined_df = combined_df.apply(zscore, axis=0)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "combined_df.to_csv('../example_data/GSE251676/log_tpm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b893b",
   "metadata": {},
   "source": [
    "### calculate the ESM embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fa77480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: NVIDIA GeForce RTX 3090 Ti\n",
      "Found 1 paired FASTA/GFF files.\n",
      "\n",
      "Processing: GCF_000011065.1\n",
      "  FASTA: ../example_data/GSE251676\\GCF_000011065.1.fna\n",
      "  GFF  : ../example_data/GSE251676\\GCF_000011065.1.gff\n",
      "  Output embeddings: ../example_data/GSE251676/GCF_000011065.1_embeddings.txt\n",
      "  Extracted 4792 protein sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Proteins: 100%|████████████████████████████████████████████████████████████████████| 4792/4792 [16:10<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 4792 embeddings to ../example_data/GSE251676/GCF_000011065.1_embeddings.txt\n",
      "\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 1) Load the ESM-2 model and set up device\n",
    "# ----------------------------------------------------------------------\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # Disables dropout (no randomness in output)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "scripts_path = os.path.join(\"..\", \"scripts\")\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "from utils import get_protein_sequences\n",
    "\n",
    "# Directory containing the paired FASTA/GFF files\n",
    "input_dir = \"../example_data/GSE251676/\"  \n",
    "\n",
    "# Retrieve lists of .fna and .gtf files\n",
    "fasta_files = glob.glob(os.path.join(input_dir, \"*.fna\"))\n",
    "gff_files = glob.glob(os.path.join(input_dir, \"*.gff\"))\n",
    "\n",
    "# Pair up files based on the base filename\n",
    "paired_files = {}\n",
    "for fasta_file in fasta_files:\n",
    "    base_name = os.path.basename(fasta_file).rsplit(\".\", 1)[0]\n",
    "    matching_gff = [\n",
    "        gff for gff in gff_files\n",
    "        if os.path.basename(gff).startswith(base_name)\n",
    "    ]\n",
    "    if matching_gff:\n",
    "        paired_files[base_name] = {\n",
    "            \"fasta\": fasta_file,\n",
    "            \"gff\": matching_gff[0]\n",
    "        }\n",
    "\n",
    "print(f\"Found {len(paired_files)} paired FASTA/GFF files.\")\n",
    "\n",
    "files_processed = 0\n",
    "# Process each pair\n",
    "for base_name, file_dict in paired_files.items():\n",
    "    genome_file = file_dict[\"fasta\"]\n",
    "    annotation_file = file_dict[\"gff\"]\n",
    "    output_file = os.path.join(input_dir, f\"{base_name}_embeddings.txt\")\n",
    "\n",
    "    print(f\"\\nProcessing: {base_name}\")\n",
    "    print(f\"  FASTA: {genome_file}\")\n",
    "    print(f\"  GFF  : {annotation_file}\")\n",
    "    print(f\"  Output embeddings: {output_file}\")\n",
    "\n",
    "    # If embeddings file already exists, skip\n",
    "    if os.path.exists(output_file):\n",
    "        print(\"  Embedding file already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # 3.1) Extract protein sequences\n",
    "#         protein_seqs = get_protein_sequences(genome_file, annotation_file)\n",
    "        protein_seqs, gene_names, gene_lengths, amino_acid_proportions = \\\n",
    "            get_protein_sequences(genome_file, annotation_file)\n",
    "        print(f\"  Extracted {len(protein_seqs)} protein sequences.\")\n",
    "\n",
    "        # 3.2) Generate embeddings with a progress bar\n",
    "        seq_repr_list = []\n",
    "        for idx, seq in tqdm(\n",
    "            enumerate(protein_seqs, start=1),\n",
    "            total=len(protein_seqs),\n",
    "            desc=\"Proteins\"\n",
    "        ):\n",
    "            if len(seq) >= 1500:\n",
    "                # Skip extremely long proteins\n",
    "                continue\n",
    "\n",
    "            # Prepare data for ESM\n",
    "            protein_name = f\"protein{idx}\"\n",
    "            data = [(protein_name, str(seq))]\n",
    "\n",
    "            # Convert text to tokens\n",
    "            batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "            batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "            token_reprs = results[\"representations\"][33]\n",
    "\n",
    "            # Count non-padding tokens, ignoring start and end tokens\n",
    "            seq_len = (batch_tokens != alphabet.padding_idx).sum(1)[0]\n",
    "            seq_repr = token_reprs[0, 1 : seq_len - 1].mean(0).cpu().numpy()\n",
    "            seq_repr_list.append(seq_repr)\n",
    "\n",
    "            # Free memory\n",
    "            del batch_tokens, token_reprs, results\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        # 3.3) Save the embeddings to a text file\n",
    "        if seq_repr_list:\n",
    "            embeddings_matrix = np.vstack(seq_repr_list)\n",
    "            np.savetxt(output_file, embeddings_matrix)\n",
    "            print(f\"  Saved {embeddings_matrix.shape[0]} embeddings to {output_file}\")\n",
    "        else:\n",
    "            print(f\"  No embeddings generated for {base_name}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {base_name}: {e}\")\n",
    "\n",
    "print(\"\\nProcessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea40b64",
   "metadata": {},
   "source": [
    "### generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "788a1fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (4792, 1280)\n",
      "TPM shape: (4774, 65)\n",
      "TPM columns: Index(['gene_id', 'GSM7985888_RN1', 'GSM7985889_RN2', 'GSM7985890_RN3',\n",
      "       'GSM7985891_RN4', 'GSM7985892_RN5', 'GSM7985893_RN6', 'GSM7985894_RN7',\n",
      "       'GSM7985895_RN8', 'GSM7985896_RN9', 'GSM7985897_RN10',\n",
      "       'GSM7985898_RN11', 'GSM7985899_RN12', 'GSM7985900_RN13',\n",
      "       'GSM7985901_RN14', 'GSM7985902_RN15', 'GSM7985903_RN16',\n",
      "       'GSM7985904_RN17', 'GSM7985905_RN18', 'GSM7985906_RN19',\n",
      "       'GSM7985907_RN20', 'GSM7985908_RN21', 'GSM7985909_RN22',\n",
      "       'GSM7985910_RN23', 'GSM7985911_RN24', 'GSM7985912_RN25',\n",
      "       'GSM7985913_RN26', 'GSM7985914_RN27', 'GSM7985915_RN28',\n",
      "       'GSM7985916_RN29', 'GSM7985917_RN30', 'GSM7985918_RN31',\n",
      "       'GSM7985919_RN32', 'GSM7985920_RN33', 'GSM7985921_RN34',\n",
      "       'GSM7985922_RN35', 'GSM7985923_RN36', 'GSM7985924_RN37',\n",
      "       'GSM7985925_RN38', 'GSM7985926_RN39', 'GSM7985927_RN40',\n",
      "       'GSM7985928_RN41', 'GSM7985929_RN42', 'GSM7985930_RN43',\n",
      "       'GSM7985931_RN44', 'GSM7985932_RN45', 'GSM7985933_RN46',\n",
      "       'GSM7985934_RN47', 'GSM7985935_RN48', 'GSM7985936_RN49',\n",
      "       'GSM7985937_RN50', 'GSM7985938_RN51', 'GSM7985939_RN52',\n",
      "       'GSM7985940_RN53', 'GSM7985941_RN54', 'GSM7985942_RN55',\n",
      "       'GSM7985943_RN56', 'GSM7985944_RN57', 'GSM7985945_RN58',\n",
      "       'GSM7985946_RN59', 'GSM7985947_RN60', 'GSM7985948_RN61',\n",
      "       'GSM7985949_RN62', 'GSM7985950_RN63', 'GSM7985951_RN64'],\n",
      "      dtype='object')\n",
      "Data has been filtered and saved.\n",
      "Filtered embeddings shape: (4618, 1280)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "scripts_path = os.path.join(\"..\", \"scripts\")\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "from utils import get_protein_sequences\n",
    "\n",
    "# Adjust these values as needed\n",
    "strain_id = 'GCF_000011065.1'\n",
    "folder_name = '../example_data/GSE251676/'\n",
    "\n",
    "# Input files\n",
    "genome_file = f'{folder_name}/{strain_id}.fna'\n",
    "annotation_file = f'{folder_name}/{strain_id}.gff'\n",
    "embeddings_file = f'{folder_name}/{strain_id}_embeddings.txt'\n",
    "tpm_file = f'{folder_name}/log_tpm.csv'\n",
    "\n",
    "# Output files\n",
    "filtered_tpm_file = f'{folder_name}/{strain_id}_filtered_log_tpm.csv'\n",
    "filtered_embeddings_file = f'{folder_name}/{strain_id}_filtered_embeddings.txt'\n",
    "filtered_meta_file = f'{folder_name}/{strain_id}_filtered_meta.txt'\n",
    "\n",
    "# 1) Extract protein sequences and gene metadata\n",
    "protein_sequences, gene_names, gene_lengths, amino_acid_proportions = \\\n",
    "    get_protein_sequences(genome_file, annotation_file)\n",
    "\n",
    "# 2) Prepare 'gene_meta' as [normalized length + 20 AA proportions]\n",
    "gene_lengths_array = np.array(gene_lengths, dtype=float)\n",
    "max_length = gene_lengths_array.max() if len(gene_lengths_array) > 0 else 1.0\n",
    "gene_lengths_norm = (gene_lengths_array / max_length).reshape(-1, 1)\n",
    "\n",
    "aa_props_array = np.array(amino_acid_proportions)\n",
    "gene_meta = np.hstack([gene_lengths_norm, aa_props_array])\n",
    "\n",
    "# (Optional) If you need to modify gene names (commented out):\n",
    "# gene_names = [x[:6] + \"_RS0\" + x[6:] for x in gene_names]\n",
    "# or other transformations depending on your naming scheme...\n",
    "\n",
    "# 3) Load existing ESM embeddings (matching the same order as gene_names)\n",
    "embeddings = np.loadtxt(embeddings_file)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "# 4) Load log_tpm data\n",
    "tpm = pd.read_csv(tpm_file)\n",
    "print(\"TPM shape:\", tpm.shape)\n",
    "print(\"TPM columns:\", tpm.columns)\n",
    "\n",
    "# Check your gene ID column name in 'tpm'. Suppose it's 'gene_id'.\n",
    "# Filter for overlap with 'gene_names':\n",
    "overlap_indices = tpm['gene_id'].isin(gene_names)\n",
    "\n",
    "# 5) Create a new tpm dataframe for overlapping entries\n",
    "new_tpm = tpm[overlap_indices].copy()\n",
    "\n",
    "# 6) Find corresponding indices in 'gene_names' for the overlapping genes\n",
    "#    This preserves the correct order for slicing embeddings + meta\n",
    "indices_in_x = [gene_names.index(g) for g in new_tpm['gene_id']]\n",
    "\n",
    "# 7) Slice embeddings and meta to match those overlapping indices\n",
    "new_embeddings = embeddings[indices_in_x, :]\n",
    "new_meta = gene_meta[indices_in_x, :]\n",
    "\n",
    "# 8) Save results\n",
    "new_tpm.to_csv(filtered_tpm_file, index=False)\n",
    "np.savetxt(filtered_embeddings_file, new_embeddings)\n",
    "np.savetxt(filtered_meta_file, new_meta)\n",
    "\n",
    "print(\"Data has been filtered and saved.\")\n",
    "print(\"Filtered embeddings shape:\", new_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009e18d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "transformers2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
